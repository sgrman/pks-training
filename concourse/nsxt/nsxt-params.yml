enable_ansible_debug: false # set value to true for verbose output from ansible

nsx_t_pipeline_branch: master

# format: "http://172.18.0.2"
nsx_image_webserver: "http://172.18.0.2"
ova_file_name: "nsx-unified-appliance-2.3.0.0.0.10085405.ova" #Uncomment this if downloaded file manually and placed under /home/concourse
ovftool_file_name: "VMware-ovftool-4.3.0-7948156-lin.x86_64.bundle"   #Uncomment this if downloaded file manually and placed under /home/concourse

# vCenter details to deploy the Mgmt OVAs (Mgr, Edge, Controller)
vcenter_ip: 192.168.110.22
vcenter_username: administrator@vsphere.local  # EDIT - this is for deployment of the ovas for the mgmt plane
vcenter_password: VMware1!                     # EDIT - this is for deployment of the ovas for themgmt plane
vcenter_datacenter: RegionA01               # EDIT
vcenter_datastore: RegionA01-ISCSI01-COMP01        # EDIT
vcenter_cluster: RegionA01-MGMT01                # EDIT

# NSX Manager general network settings
ntp_servers: 192.168.100.1              # EDIT
mgmt_portgroup: 'VM-RegionA01-vDS-MGMT'              # EDIT
dns_server: 192.168.110.10                 # EDIT
dns_domain: corp.local                   # EDIT
default_gateway: 192.168.110.1             # EDIT
netmask: 255.255.255.0                   # EDIT

nsx_manager_ip: 192.168.110.42
nsx_manager_username: admin
nsx_manager_password: VMware1!
nsx_manager_assigned_hostname: "nsxmgr-01a" # this hostname+dns_domain will be FQDN
nsx_manager_root_pwd: VMware1!    # Min 8 chars, upper, lower, number, special digit
nsx_manager_deployment_size: small   # Recommended for real barebones demo, smallest setup
nsx_manager_ssh_enabled: true
resource_reservation_off: true

# Compute manager credentials should be the same as above vCenter's if
# controllers and edges are to be on the same vCenter
compute_manager_username: "Administrator@vsphere.local"
compute_manager_password: "VMware1!"
# compute manager for the compute cluster (2nd vCenter)
compute_manager_2_vcenter_ip: "null"
compute_manager_2_username: "null"
compute_manager_2_password: "null"

edge_uplink_profile_vlan: 0 # For outbound uplink connection used by Edge, usually keep as 0
esxi_uplink_profile_vlan: 0 # For internal overlay connection used by Esxi hosts, usually trasnport VLAN ID

# Virtual Tunnel Endpoint network ip pool
vtep_ip_pool_cidr: 192.168.70.0/24
vtep_ip_pool_gateway: 192.168.70.1
vtep_ip_pool_start: 192.168.70.10
vtep_ip_pool_end: 192.168.70.200

# Tier 0 router
tier0_router_name: t0-pks
tier0_uplink_port_ip: 192.168.71.8
tier0_uplink_port_subnet: 24
tier0_uplink_next_hop_ip: 192.168.71.1
tier0_uplink_port_ip_2: 192.168.71.9
tier0_ha_vip: 192.168.71.10

## Controllers
controller_ips: 192.168.64.21 #comma separated based on number of required controllers
controller_default_gateway: 192.168.64.1
controller_ip_prefix_length: 24
controller_hostname_prefix: vlab-ctrl # Generated hostname: controller_1.corp.local.io
controller_cli_password: "VMware1!" # Min 8 chars, upper, lower, num, special char
controller_root_password: "VMware1!"
controller_deployment_size: "SMALL"
vc_datacenter_for_controller: vlab-dc
vc_cluster_for_controller: vlab-mgmt
vc_datastore_for_controller: vlab-nfs-ds-ssd
vc_management_network_for_controller: "vlab-mgmt"
controller_shared_secret: "VMware1!VMware1!"

## Edge nodes
edge_ips: 192.168.64.24,192.168.64.25    #comma separated based in number of required edges
edge_default_gateway: 192.168.64.1
edge_ip_prefix_length: 24
edge_hostname_prefix: vlab-edge
edge_transport_node_prefix: edge-tn
edge_cli_password: "VMware1!"
edge_root_password: "VMware1!"
edge_deployment_size: "medium" #Large recommended for PKS deployments
vc_datacenter_for_edge: vlab-dc
vc_cluster_for_edge: vlab-mgmt
vc_datastore_for_edge: vlab-nfs-ds-ssd
vc_uplink_network_for_edge: "nsx-uplink"
vc_overlay_network_for_edge: "nsx-transport"
vc_management_network_for_edge: "vlab-mgmt"

## ESX hosts
#Intsll vSphere clusters automatically
clusters_to_install_nsx: vlab-mgmt,vlab-cna    #Comma seprated
per_cluster_vlans: 70,70  #Comma seprated, order of VLANs applied same as order of clusters

esx_ips: "" # additional esx hosts, if any, to be individually installed
esx_os_version: "6.5.0"
esx_root_password: "VMware1!"
esx_hostname_prefix: "vlab-esx"

esx_available_vmnic: "vmnic1" # comma separated physical NICs, applies to both cluster installation or ESXi installation

# Name the routers and logical switches and cidrs differently to avoid conflict
nsx_t_t1router_logical_switches_spec: |
  t1_routers:
  # Add additional T1 Routers or collapse switches into same T1 Router as needed
  # Comment off the following T1 Routers if there is no PKS
  - name: t1-pks-mgmt
    switches:
    - name: ls-pks-mgmt
      logical_switch_gw: 172.31.0.1 # Last octet should be 1 rather than 0
      subnet_mask: 24
  - name: t1-pks-service
    switches:
    - name: ls-pks-service
      logical_switch_gw: 172.31.2.1 # Last octet should be 1 rather than 0
      subnet_mask: 24

nsx_t_ha_switching_profile_spec: |
  ha_switching_profiles:
  - name: HASwitchingProfile

# Add additional container ip blocks as needed
nsx_t_container_ip_block_spec: |
  container_ip_blocks:
  - name: ip-block-pods-deployments
    cidr: 172.16.0.0/16
  - name: ip-block-nodes-deployments
    cidr: 10.16.0.0/16

# Add additional exernal ip pools as needed
nsx_t_external_ip_pool_spec: |
  external_ip_pools:
  - name: ip-pool-vips
    cidr: 192.168.75.0/24
    gateway: 192.168.75.1
    start: 192.168.75.10 # Should not include gateway
    end: 192.168.75.254  # Should not include gateway


# Specify NAT rules
# Provide matching dnat and snat rule for specific vms by using ips for destination_net work and translated_network that need to be exposed like Ops Mgr
# Provide snat rules for outbound from either container or a vm by specifying the source_network (cidr) and translated network ip
nsx_t_nat_rules_spec: |
  nat_rules:
  # Sample entry for PKS MGMT and Service networks
  - t0_router: t0-pks
    nat_type: snat
    source_network: 172.31.0.0/24         # PKS Clusters network cidr
    translated_network: 192.168.76.1      # SNAT External Address for PKS networks
    rule_priority: 1024                   # Higher priority

  # Sample entry for allowing inbound to PKS Ops manager
  - t0_router: t0-pks
    nat_type: dnat
    destination_network: 192.168.76.2     # External IP address for PKS opsmanager
    translated_network: 172.31.0.2        # Internal IP of PKS Ops manager
    rule_priority: 1024                   # Higher priority

  # Sample entry for allowing inbound to BOSH
  - t0_router: t0-pks
    nat_type: dnat
    destination_network: 192.168.76.3     # External IP address for PKS opsmanager
    translated_network: 172.31.0.3        # Internal IP of PKS Ops manager
    rule_priority: 1024                   # Higher priority

  # Sample entry for allowing inbound to PKS Controller
  - t0_router: t0-pks
    nat_type: dnat
    destination_network: 192.168.76.4    # External IP address for PKS Controller
    translated_network: 172.31.0.4       # Internal IP of PKS Ops Controller
    rule_priority: 1024                  # Higher priority

  # Sample entry for allowing inbound to Harbor
  - t0_router: t0-pks
    nat_type: dnat
    destination_network: 192.168.76.5    # External IP address for Harbor
    translated_network: 172.31.0.5       # Internal IP of Harbor
    rule_priority: 1024                  # Higher priority

nsx_t_csr_request_spec: |
  csr_request:
    #common_name not required - would use nsx_t_manager_host_name
    org_name: VMware             # EDIT
    org_unit: vlab               # EDIT
    country: US                  # EDIT
    state: CO                    # EDIT
    city: Springs                # EDIT
    key_size: 2048               # Valid values: 2048 or 3072
    algorithm: RSA               # Valid values: RSA or DSA

# LBR definition
# By default, all the server pools would use Layer 4 - TCP pass through
# No ssl termination or handling, uses default tcp active monitor & auto map for virtual server
# Auto Map mode uses LB interface IP and ephemeral port.
# In scenarios where both Clients and Pool Members are attached to the same Logical Router,
# SNAT (Auto Map or IP List) must be used.

# LBR Sizing guide
# LB Size | Virtual Servers | Pool Members
# small   |   10            |   30
# medium  |  100            |  300
# large   | 1000            | 3000

# No. of LBs per edge based on size of edge
# Edge Size | Small LBs| Medium LBs| Large LBs
#
# small     |    0     |   0       | 0
# medium    |    1     |   0       | 0 # Recommended for running only PAS or PKS
# large     |    4     |   1       | 1 # Recommended for running PAS + PKS
# Bare metal - not handled here
nsx_t_lbr_spec:
#nsx_t_lbr_spec: |
#  loadbalancers:
  # Sample entry for creating LBR for PAS ERT
#  - name: PAS-ERT-LBR
#    t1_router: T1-Router-PAS-ERT # Should match a previously declared T1 Router
#    size: small                  # Allowed sizes: small, medium, large
#    virtual_servers:
#    - name: goRouter443         # Name that signifies function being exposed
#      vip: 10.208.40.4         # Exposed VIP for LBR to listen on
#      port: 443
#      members:
#      - ip: 192.168.24.11       # Internal ip of GoRouter instance 1
#        port: 443
#    - name: goRouter80
#      vip: 10.208.40.4
#      port: 80
#      members:
#      - ip: 192.168.24.31       # Internal ip of GoRouter instance 1
#        port: 80
#      - ip: 192.168.24.32       # Internal ip of GoRouter instance 2
#        port: 80
#    - name: sshProxy            # SSH Proxy exposed to outside
#      vip: 10.208.40.5
#      port: 2222                # Port 2222 for ssh proxy
#      members:
#      - ip: 192.168.24.41       # Internal ip of Diego Brain where ssh proxy runs
#        port: 2222
